---
title: "The Receiving End of the Cooperative Eye Hypothesis"
author: "M Schmettow, C. Willemse, S. Borsci"
format: html
editor: visual
---

## Introduction

The human eye region has a central role in non-verbal communication. Next to emotional expressions, humans are very accurate and fast at reading each others glance directions. The Cooperative Eye Hypothesis (CEH) states that this ability has evolved, when human ancestors developed their ability to cooperate and show genuine altruistic behaviour. Initially, the hypothesis is based on the observation that Homo Sapiens (HS) is the only species among extant Great Apes, which has developed a visible white sclera. The first argument in this paper is that under the CEH it is very likely that the eye ball evolves into a form that is *computationally efficient*. The ability to establish shared attention by glance perception may have also played a role in child rearing, which suggests that a *reciprocal* innate ability to read glance direction may have evolved alongside.

Eye tracking devices are a instruments to observe glance directions. From the CEH it follows that the eye ball is sending strong signals, such that a device to read glance directions should be easy to construct, if just the cognitive mechanism was known. Under the CEH, effective eye tracking must be possible using only visual information that also is available to a human receiver. In contrast, recent eye tracking technology often relies on corneal reflections, which is not what humans usually see and thus not cannot be biomimetic.

The problem with building a biomimetic eye tracker is that a precise cognitive model of glance perception is not available. However, it is possible to build an eye tracker, that can be called *bioconvergent*, in that it may work differently in detail, but only with information available also to human receivers. In this paper we present a bioconvergent method to eyetracking. The *QuadBright* method uses the brightness distribution of the image and simple statistical learning to establish eye tracking. Using a prototype, we evaluate the accuracy of the eye tracking device and compare it to the accuracy of common human observers.

Once a bioconvergent eye tracking device is established, it is possible to test an extended version of the CEH, which states that human glance perception is an innate cognitive ability that evolved in reciprocity with eye ball structure. In the experiment, we measure participants glance direction performance, when they only saw the information available to the QuadBright algorithm. If human performance is unhindered by the degrading, it can be concluded, that underlying computation is similar.

### The Cooperative Eye Hypothesis (CEH)

In fact, all animal scleras are white, which is occluded by a layer of pigmentation in the conjunctiva. As KK note, sustaining pigmentation costs the organism energy. From a biophysical perspective it takes a very precise molecular configuration to produce exact colors. The biosynthetic pathways of melanin are complex and several mutations are known, such as albinism, and bright iris color  (Eiberg et al., 2008). Non fatal single-point mutations are possible and could have benefits for th eorganism, the real question therefore seems to be: why have Great Apes and many other animals dark scleras? 

One explanation is that protection against predators. Since the eyeball is an optical device in the first place, a pupil is dark almost inevitable. In its default state, the animal eye is a high-contrast target, that is also rapidly moving in the case of Great Apes, and the conjunctiva pigmentation may have evolved as camouflage (KK). Another hypothesis is that within-species competition is the driving force for dark scleras in the Great Apes.


### Origins of Glance Reading

The possibility of a single-point mutation producing a white sclera surely exists and examples of innate social signals between animals exist, too, for example in wolves direct staring is a signal of challenge, or when cats narrow their eyes in a "smile". Essentially, the CEH states that the human eye has evolved to be a signalling device. However, all within-species and even some between-species signalling devices can only evolve, when there is a *reciprocal cognitive mechanism to receive* the signal. 
The main question in this paper is: What was on the receiving end, when white scleras started to occur? What is the origin of the ability to read glance directions, and how does it work?

In modern humans, glance reading almost certainly is an innate ability, as it starts to develop in the first year of life. If within-species deception was the driving factor behind dark scleras, this implies that a cognitive receiver mechanism has existed all along, and must have evolved for a different purpose. Hoffmann compares human visual processing to a computer user interface, which deceivingly hides the true nature of reality, but does so in a way useful for reproduction.

Examples for specialized visual processing modes exist, for example in the many ways how male animals perform courtship displays. The human visual system is also specialized to detect certain features, such as faces and primary sexual features. As evolutionary design is inventive only in the sense of re-using existing structures, visual processing modes that appear very specialized have evolved from existing structures and must therefore be very old. As the faces of vertebrates have the same basic structure, there was plenty of time for specialized face recognition mechanisms to evolve, and not just in humans. As the unpigmented human sclera is much younger and may have even have appeared as a sudden mutation, it must have hit on an existing cognitive structure to be effective. 


### Cognitive mechanisms glance perception


Yorzinski et al. (2021, 10.3389/fpsyg.2021.632616) evaluated the glance reading performance in various conditions, including pigmentation of sclera and iris color. The found a strong increase in latency in the combination of pigmented sclera with a dark iris. More interestingly for the quest of the receiving mechanism is that the accuracy was only affected very slightly. Overall, this experiment shows that glance reading is robust under a variety of conditions.

[TODO]: Describe psychophysical properties

...

Some quite generic visual processing modes are commonly known as gestalts, and these have evolved because they match some common patterns in the physical and the biome. For example, the Symmetry gestalt has evolved, because (mirror) symmetry is the most striking feature of practically all extant animals, and therefore is a highly relevant cue. Forming an approximate sphere, the eyeball is strongly symmetric, which is a functional requirement of optical systems, as well as of keeping pressurized  fluid in a container. It may be a co-incidence, but its original shape and the makes the eyeball amenable for quick and accurate visual processing as provided by Symmetry.





<!-- The stare-in-the-crowd phenomenon shows that human glance perception is accurate even over a distance of several meters. An eye region of 40mm horizontally in 3 meter distance has a perceived visual angle of around 45 arc min. If we further take the mobility of the eye ball to span 4500 arc min (75 degree) and foveal resolution of 1 arc min on the receiving end, the *theoretical* resolution limit for glance direction is approximately 100 arc min (1.7 degree). Compared to that, the horizontal visual angle of the receivers face is 300 arc min. -->

<!-- Under the same circumstances, the senders corneal reflections span 6 arc min on the receiving end and the same calculation produces a resolution limit of 900 arc min (15 degree). It is immediately clear that corneal reflections are not biomimetic, as this would never allow glance perception at a distance. -->

<!-- ### Bio-convergent Eye Tracking -->



## Bioconvergent eye tracking

If the eye ball sends such strong signals that humans can still read glance directions in highly degraded images, then it must be possible to build an effective eye tracking device that uses the same information. While not guaranteed, it is possible that designing an eye tracking algorithm under these constraints results in a good model, how human glance reading works.

From the characteristics of human glance reading, the following *bioconvergence requirements* can be derived:

1. The algorithm must be able to work with a very low resolution image, as the human eye is able to read glance directions even in highly degraded images and from a distance.
1. The algorithm must require only minimal calibration.
1. The algorithm must be able to work in a wide range of lighting conditions.
1. The algorithm must use only generic features of the image
1. The algorithm must be computationally efficient

The initial idea for such an algorithm was derived from the observation of symmetry of the eye ball. While similar approaches attempt to identify the position of the pupil by using computer vision algorithms, this did not seem to satisfy the requirements above.

The QuadBright method is derived from the observation, that the moving pupil produces a change in *spatial brightness distribution*. From an algorithmic point, symmetry operations are memory efficient and fast to compute. The image is split in four equally spaced quadrants  NE, SE, SW and NW (Fig X). By reducing every quadrant to its average brightness, the image resolution is reduced to 2-by-2 pixels. During calibration, average quadrant brightness measures are collected at nine calibration points, and a multiple linear regresion model is trained, for x and y directions separately. The trained model is then used to predict eye ball position from brightness quadrant data in real time.

$$
y_i = \beta_0 + \beta_1 x_\textrm{NE} + \beta_2 x_\textrm{SE} + \beta_3 x_\textrm{SW} + \beta_4 x_\textrm{NW}
$$

### YET Zero prototype

Given that QuadBright essentially compresses the input into a four pixel frame, a high resolution camera is not necessary. More important is a small footprint, as it had to be mounted in frontal position. The choice fell on a type of commercially available USB endoscope cameras. These cameras with a diameter of 5.5mm create almost no obstruction when mounted in the visual field and the built-in LED lights provide a stable light source for better accuracy. On the downside these devices deliver a poor resolution of 480x320 and use unspecified electronic components. A simple 3D-printed part was created to be able to glue the camera to a stick, which in turn is connected to a head mount.

The YET Zero application was written in Python, using OpenCV for image processing and PyGame for the GUI. The regression model was implemented using the scikit-learn library. First the user is asked to adjust the camera position to be approximately centered. An eye detection algorithm marks the eye region as input for quadrant brightness. During calibration, nine points are shown to the participant and quadrant brightness is used to train the regression model, which provides real time eye positions estimates. To compensate for minor drifts, every stimulus is preceded by a quick calibration routine, which only records the center point to apply horizontal and vertical translations.

This system already worked well, but it turned out that changes in computer screen brightness effects can introduce estimation biases. For compensation, the quick calibration step was augmented with a highly degraded preview of the next stimulus, just enough to keep the brightness stable between quick calibration and stimulus presentation.

## Results

## Study 1: Accuracy of the QuadBright algorithm

The first study was conducted to measure the accuracy of the Quadbright algorithm, with the expectation that it matchges human performance.
In the experiment participants were asked to follow a target, while the QuadBright algorithm tracked their eye positions. Accuracy was measured by calculating the angular accuracy of the predicted positions.

### Design and Sample

The first study was conducted to evaluate the accuracy of the QuadBright algorithm. The algorithm was tested on a set of 36 participants, who were asked to find and focus an orange target on a white background, moving in a random pattern between 24 positions. 

The experiment was carried out with Yet Zero. The headmount was constructed from an old headphone and a kitchen paper roll was used as a chin rest. Participants were seated in 45cm viewing distance using a 16:9 screen with a 41cm diagonal. The experiment consisted of 24 trials, each lasting 3 seconds. The target moved in a random pattern between 24 positions on the screen. The participants were asked to follow the target with their eyes. The QuadBright algorithm tracked their eye movements and the accuracy of the algorithm was measured by calculating the angular standard error of predicted eye movements of the participants. 

The original experiment tested various lighting conditions (LED, IR, no light), as well as the before-mentioned bias due to changing screen brightness levels ((FB24)). The latter was confirmed to such an extent that the results were discarded. The three levels of lighting conditions differed so little that for the analysis here data was pooled.


```{r echo = F, output = F}
library(tidyverse)
load("FB/FB24.rda")

# first timestamp recorded
t_offset <- min(final_data$Part)
n_sessions <- 6

FB24_0 <-
  final_data |> 
  ungroup() |>
  mutate(BG = str_extract(Stim, "White|Black"),
         Target = str_extract(Stim, "[A-Z]\\d")) |> 
  select(-Participant, -Condition) |>
  ## Observation meta data
  mutate(start_time = Part - t_offset,
         Run = as.integer(as.factor(as.integer(start_time))),
         Part = Run %/% n_sessions + 1,
         Session = Run %% n_sessions + 1,
         time = time - t_offset - start_time) |> 
  ## Finding trials
  group_by(Part, Session, Target, BG) |> 
  mutate(new_trial = Stim != lag(Stim, default = "_0"),
         Rep = cumsum(new_trial) - 1) |>
  fill(Rep) |> 
  ungroup() |> 
  group_by(Part) |> 
  mutate(trial = cumsum(new_trial)) |> 
  ungroup() |> 
  group_by(Part, trial) |>
  mutate(time = time - min(time)) |> 
  ungroup() |> 
  select(Run, Part, Session, trial, start_time, Rep, Target, target_x, target_y, time, BG, x, y, x_pro, y_pro, visual_angle_x:visual_angle_accuracy)

summary(FB24_0)

FB24_0 |> 
  ggplot(aes(x = time, y = visual_angle_accuracy, col = as.factor(Session))) +
  geom_smooth(se = F) +
  facet_wrap(~Part) +
  theme_minimal()

## only using middle second
FB24 <- 
  FB24_0 |> 
  filter(BG == "White" & time > 1 & time < 2)
```


```{r eval = F, echo = F}
FB24 |> 
  group_by(Part, Session) |>
  summarize(`mean angular error` = mean(visual_angle_accuracy)) |> 
  ggplot(aes(x = Session, y = `mean angular error`, group = as.factor(Part))) + geom_line()

```

```{r fig.height=10, eval = F, echo = F}
FB24 |> 
  group_by(Part, target_x, target_y, Session) |> 
  summarize(mean_error = mean(visual_angle_accuracy)) |> 
  ungroup() |> 
  ggplot(aes(color = as.factor(Session), x = mean_error)) +
  geom_density() +
  facet_grid(target_y ~ target_x) +
  scale_x_log10()

```


```{r}
FB24_1 <- 
  FB24 |> 
    group_by(Part, target_x, target_y, trial) |> 
    summarize(accuracy = mean(visual_angle_accuracy)) |> 
    ungroup()

FB24_1 |> 
  group_by(target_x, target_y) |>
  summarize(mean_error = mean(accuracy), sd_error = sd(accuracy), median = median(accuracy))
```


Figure ((XZ)) shows the distribution of angular errors combined (on $\log_{10}$ scale). The median accuracy is 2.8 degree, with a mean of 3.2 degree. 95 percent of all errors are below 8.5 degree. The accuracy is best in the center region with a median of 1.3 - 1.4 degree accuracy. The error increases towards the edges of the screen, but asymmetrically. To the mount side the median error is 2.6 - 3.7 degree, but 5 - 5.5 degrees to the inner side of the eye. In all positions, errors larger than 10 degree were extremely rare. We can conclude that the QuadBright algorithm is able to estimate eye ball positions reasonably accurate and robust under different light conditions. The few catastrophes mostly occurred on three (out of 36) participants, for which the calibration didn't seem to work reliably.


```{r}
T_0 <- 
  FB24_1 |> 
  summarize(mean(accuracy), median(accuracy), quantile(accuracy, .95))
  

FB24_1 |> 
  filter(accuracy < 20) |> 
  ggplot(aes(x = accuracy)) +
#    facet_grid(target_y ~ target_x) +
  scale_x_log10() +
  geom_histogram() +
  geom_vline(aes(xintercept = median(accuracy), col = "median")) +
  geom_vline(aes(xintercept = mean(accuracy), col = "mean")) +
  geom_vline(aes(xintercept = quantile(accuracy,.95), col = "95% quant."))
  

```


```{r fig.width = 12, fig.height = 10}


 FB24_1 |> 
    filter(accuracy < 20) |> 
    ggplot(aes(x = accuracy)) +
    facet_grid(target_y ~ target_x) +
    geom_density()

```

Figure ((XY)) shows the distribution of angular errors per target position across all trials. Accuracy is best in the center region with a median of 1.3 - 1.4 degree accuracy. The error increases towards the edges of the screen, but asymmetrically. To the mount side the median error is 2.6 - 3.7 degree, but 5 - 5.5 degrees to the inner side of the eye. In all positions, errors larger than 10 degree were extremely rare. We can conclude that the QuadBright algorithm is able to estimate eye ball positions reasonably accurate and robust under different light conditions. The few catastrophes mostly occurred on three (out of 36) participants, for which the calibration didn't seem to work reliably.


### Human performance with Quadbright information

## Discussion

The Cooperative Eye Hypothesis states that the eyeball has evolved to be

### Performance



### Signal efficiency

Social signalling devices evolve in reciprocity, as many examples from animal sexual and caregiving behaviour show. In human evolution, caregivers evolved an urge to smile at infants, which in turn evolved to see smiles ((REF)). Modern communication has reduced human smiling to its geometric core. Two dots and an arc can make a baby happy.
Evolution does not have the power to simultaneously evolve new pairs of senders and a receivers. 
Absence of certain pigments is a common type of genetic disorder. Also, pigmentation consumes energy, which made XY(19xx) conclude that the advantage of pigmentation in the sclera yields is *deception*. It is easy to imagine how a white sclera spontaneously appears, but what made it stay? From the reciprocity condition follows that the initial signal must have matched with existing cognitive structures in the receiver. 

Simple signals have a higher chance of hitting on existing structures. The fact that an eye tracking device only needs four pixel, 10 parameters and the four arithmetic operations to work, proves that the human eyeball is sending a simple and clear signal.


### The Reciprocal Cooperative Eye theory

How can a woman say with certainty that a man in 3m distance is looking at her and not at the leopard in the tree behind her? The theoretical limits we calculated suggest that everyday human glance perception is operating at the limits of the visual system. Even if the eye ball has evolved into the perfect sender for glance direction signals, it is likely that specialized cognitive functions have evolved on the receiving end.

Our second experiment showed that human glance perception is barely affected by pixelized eyes. This suggests that the underlying cognitive mechanism is similar to the QuadBright algorithm.

### Future research

Our search for a candidate biomimetic algorithm was short and successful. Other algorithms may exist and should be explored. Furthermore, the experiment confirms biomimesis of QB only insofar as human processing can perform with the same information. A stronger test for biomimesis is to create situations, where QB is likely to fail, for example, when strong reflections distort the perceived brightness distribution, or when the eye is highly asymmetric. Also, QB ignores any curvature, which produces biases in extreme eyeball positions. If the device is biomimetic, similar biases should be observable.

### Conclusion
