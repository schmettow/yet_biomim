---
title: "Testing the Cooperative Eye Hypothesis by bioconvergent eye tracking"
author: "M Schmettow, C. Willemse, S. Borsci"
format: html
editor: visual
---

## Introduction

The human eye region has a central role in non-verbal communication. Next to emotional expressions, humans are very accurate and fast at reading each others glance directions. The Cooperative Eye Hypothesis (CEH) states that this ability has evolved, when human ancestors developed their ability to cooperate and show genuine altruistic behaviour. Initially, the hypothesis is based on the observation that Homo Sapiens (HS) is the only species among extant Great Apes, which has developed a visible white sclera. The first argument in this paper is that under the CEH it is very likely that the eye ball evolves into a form that is *computationally efficient*. The ability to establish shared attention by glance perception may have also played a role in child rearing, which suggests that a *reciprocal* innate ability to read glance direction may have evolved alongside.

Eye tracking devices are a instruments to observe glance directions. From the CEH it follows that the eye ball is sending strong signals, such that a *biomimetic* device to read glance directions should be easy to construct, if just the cognitive mechanism was known. Under the CEH, effective eye tracking must be possible using only visual information that also is available to a human receiver. In contrast, recent eye tracking technology often relies on corneal reflections, which is not what humans usually see and thus not cannot be biomimetic.

The problem with building a biomimetic eye tracker is that a precise cognitive model of glance perception is not available. However, it is possible to build an eye tracker, that can be called *bioconvergent*, in that it may work differently in detail, but only with information available also to human receivers. In this paper we present a bioconvergent method to eyetracking. The *QuadBright* method uses the brightness distribution of the image and simple statistical learning to establish eye tracking. Using a prototype, we evaluate the accuracy of the eye tracking device and compare it to the accuracy of common human observers.

Once a bioconvergent eye tracking device is established, it is possible to test an extended version of the CEH, which states that human glance perception is an innate cognitive ability that evolved in reciprocity with eye ball structure. In the experiment, we measure participants glance direction performance, when they only saw the information available to the QuadBright algorithm. If human performance is unhindered by the degrading, it can be concluded, that underlying computation is similar.

### The Cooperative Eye Hypothesis

### Bio-convergent Eye Tracking

If the eye ball has evolved to send strong signals as the CEH states, then it must be possible to build an eye tracking device that

-   uses only information available to a human receiver
-   is computationally efficient

The first aim of this work is to develop a bio-convergent eye tracking device, which uses visible light only and has similar characteristics as human glance perception.

### Theoretical limits of glance perception

The stare-in-the-crowd phenomenon shows that human glance perception is accurate even over a distance of several meters. An eye region of 40mm horizontally in 3 meter distance has a perceived visual angle of around 45 arc min. If we further take the mobility of the eye ball to span 4500 arc min (75 degree) and foveal resolution of 1 arc min on the receiving end, the *theoretical* resolution limit for glance direction is approximately 100 arc min (1.7 degree). Compared to that, the horizontal visual angle of the receivers face is 300 arc min.

Under the same circumstances, the senders corneal reflections span 6 arc min on the receiving end and the same calculation produces a resolution limit of 900 arc min (15 degree). It is immediately clear that corneal reflections are not biomimetic, as this would never allow glance perception at a distance.

## QuadBright: A bioconvergent eye tracking method

The QuadBright method is derived from the observation, that eye movements produce a change in spatial *brightness distribution* by the moving iris. Another idea during the design was that in frontal projection, the human eye is almost symmetric in the horizontal and vertical axis. The human visual system is strongly tuned to detect *symmetry*, as is documented by the Gestalt law. From an algorithmic point, symmetry operations are memory efficient and fast to compute. Lastly, computational efficiency can only be claimed if the algorithm uses simple operations and few parameters. Finally, glance perception usually happens from a more or less *frontal perspective*.

First, QuadBright fulfills these requirements by drastically degrading the input signal. Every frame is split in four quadrants NE, SE, SW and NW and is reduced to average brightness. During calibration, average quadrant brightness measures are produced with the true location $y_i$ available. This data is send to a regression engine using a multiple linear regression model. This is done separately for x and y directions, not using any conditional terms. The model is trained on the calibration data and then used to predict eye ball position in real time.

$$
y_i = \beta_0 + \beta_1 x_\textrm{NE} + \beta_2 x_\textrm{SE} + \beta_3 x_\textrm{SW} + \beta_4 x_\textrm{NW}
$$

### YET Zero prototype

Given that QuadBright essentially compresses the input into a four pixel frame, a high resolution camera is not necessary. More important is a small footprint, as it had to be mounted in frontal position. The choice fell on a type of commercially available USB endoscope cameras. These cameras with a diameter of 5.5mm create almost no obstruction when mounted in the visual field and the built-in LED lights provide a stable light source for better accuracy. On the downside these devices deliver a poor resolution of 480x320 and use unspecified electronic components. A simple 3D-printed part was created to be able to glue the camera to a stick, which in turn is connected to a head mount.

The YET Zero application was written in Python, using OpenCV for image processing and PyGame for the GUI. The regression model was implemented using the scikit-learn library. First the user is asked to adjust the camera position to be approximately centered. An eye detection algorithm marks the eye region as input for quadrant brightness. During calibration, nine points are shown to the participant and quadrant brightness is used to train the regression model, which provides real time eye positions estimates. To compensate for minor drifts, every stimulus is preceded by a quick calibration routine, which only records the center point to apply horizontal and vertical translations.

This system already worked well, but it turned out that changes in computer screen brightness effects can introduce estimation biases. For compensation, the quick calibration step was augmented with a highly degraded preview of the next stimulus, just enough to keep the brightness stable between quick calibration and stimulus presentation.

## Results

## Study 1: Accuracy of the QuadBright algorithm

The first study was conducted to measure the accuracy of the Quadbright algorithm, with the expectation that it matchges human performance.
In the experiment participants were asked to follow a target, while the QuadBright algorithm tracked their eye positions. Accuracy was measured by calculating the angular accuracy of the predicted positions.

### Design and Sample

The first study was conducted to evaluate the accuracy of the QuadBright algorithm. The algorithm was tested on a set of 36 participants, who were asked to find and focus an orange target on a white background, moving in a random pattern between 24 positions. 

The experiment was carried out with Yet Zero. The headmount was constructed from an old headphone and a kitchen paper roll was used as a chin rest. Participants were seated in 45cm viewing distance using a 16:9 screen with a 41cm diagonal. The experiment consisted of 24 trials, each lasting 3 seconds. The target moved in a random pattern between 24 positions on the screen. The participants were asked to follow the target with their eyes. The QuadBright algorithm tracked their eye movements and the accuracy of the algorithm was measured by calculating the angular standard error of predicted eye movements of the participants. 

The original experiment tested various lighting conditions (LED, IR, no light), as well as the before-mentioned bias due to changing screen brightness levels ((FB24)). The latter was confirmed to such an extent that the results were discarded. The three levels of lighting conditions differed so little that for the analysis here data was pooled.


```{r echo = F, output = F}
library(tidyverse)
load("FB/FB24.rda")

# first timestamp recorded
t_offset <- min(final_data$Part)
n_sessions <- 6

FB24_0 <-
  final_data |> 
  ungroup() |>
  mutate(BG = str_extract(Stim, "White|Black"),
         Target = str_extract(Stim, "[A-Z]\\d")) |> 
  select(-Participant, -Condition) |>
  ## Observation meta data
  mutate(start_time = Part - t_offset,
         Run = as.integer(as.factor(as.integer(start_time))),
         Part = Run %/% n_sessions + 1,
         Session = Run %% n_sessions + 1,
         time = time - t_offset - start_time) |> 
  ## Finding trials
  group_by(Part, Session, Target, BG) |> 
  mutate(new_trial = Stim != lag(Stim, default = "_0"),
         Rep = cumsum(new_trial) - 1) |>
  fill(Rep) |> 
  ungroup() |> 
  group_by(Part) |> 
  mutate(trial = cumsum(new_trial)) |> 
  ungroup() |> 
  group_by(Part, trial) |>
  mutate(time = time - min(time)) |> 
  ungroup() |> 
  select(Run, Part, Session, trial, start_time, Rep, Target, target_x, target_y, time, BG, x, y, x_pro, y_pro, visual_angle_x:visual_angle_accuracy)
  
FB24_0 |> 
  ggplot(aes(x = time, y = visual_angle_accuracy, col = as.factor(Session))) +
  geom_smooth(se = F) +
  facet_wrap(~Part) +
  theme_minimal()

## only using middle second
FB24 <- 
  FB24_0 |> 
  filter(BG == "White" & time > 1 & time < 2)
```


```{r eval = F, echo = F}
FB24 |> 
  group_by(Part, Session) |>
  summarize(`mean angular error` = mean(visual_angle_accuracy)) |> 
  ggplot(aes(x = Session, y = `mean angular error`, group = as.factor(Part))) + geom_line()

```

```{r fig.height=10, eval = F, echo = F}
FB24 |> 
  group_by(Part, target_x, target_y, Session) |> 
  summarize(mean_error = mean(visual_angle_accuracy)) |> 
  ungroup() |> 
  ggplot(aes(color = as.factor(Session), x = mean_error)) +
  geom_density() +
  facet_grid(target_y ~ target_x) +
  scale_x_log10()

```


```{r}
FB24_1 <- 
  FB24 |> 
    group_by(Part, target_x, target_y, trial) |> 
    summarize(accuracy = mean(visual_angle_accuracy)) |> 
    ungroup()

FB24_1 |> 
  group_by(target_x, target_y) |>
  summarize(mean_error = mean(accuracy), sd_error = sd(accuracy), median = median(accuracy))
```



```{r fig.width = 12, fig.height = 10}


 FB24_1 |> 
    filter(accuracy < 20) |> 
    ggplot(aes(x = accuracy)) +
    facet_grid(target_y ~ target_x) +
    geom_density()

```

Figure ((XY)) shows the distribution of angular errors per target position across all trials. Accuracy is best in the center region with a median of 1.3 - 1.4 degree accuracy. The error increases towards the edges of the screen, but asymmetrically. To the mount side the median error is 2.6 - 3.7 degree, but 5 - 5.5 degrees to the inner side of the eye. In all positions, errors larger than 10 degree were extremely rare. We can conclude that the QuadBright algorithm is able to estimate eye ball positions reasonably accurate and robust.


### Human performance with Quadbright information

## Discussion

The Cooperative Eye Hypothesis states that the eyeball has evolved to be

### Performance



### Signal efficiency

Social signalling devices evolve in reciprocity, as many examples from animal sexual and caregiving behaviour show. In human evolution, caregivers evolved an urge to smile at infants, which in turn evolved to see smiles ((REF)). Modern communication has reduced human smiling to its geometric core. Two dots and an arc can make a baby happy.
Evolution does not have the power to simultaneously evolve new pairs of senders and a receivers. 
Absence of certain pigments is a common type of genetic disorder. Also, pigmentation consumes energy, which made XY(19xx) conclude that the advantage of pigmentation in the sclera yields is *deception*. It is easy to imagine how a white sclera spontaneously appears, but what made it stay? From the reciprocity condition follows that the initial signal must have matched with existing cognitive structures in the receiver. 

Simple signals have a higher chance of hitting on existing structures. The fact that an eye tracking device only needs four pixel, 10 parameters and the four arithmetic operations to work, proves that the human eyeball is sending a simple and clear signal.


### The Reciprocal Cooperative Eye theory

How can a woman say with certainty that a man in 3m distance is looking at her and not at the leopard in the tree behind her? The theoretical limits we calculated suggest that everyday human glance perception is operating at the limits of the visual system. Even if the eye ball has evolved into the perfect sender for glance direction signals, it is likely that specialized cognitive functions have evolved on the receiving end.

Our second experiment showed that human glance perception is barely affected by pixelized eyes. This suggests that the underlying cognitive mechanism is similar to the QuadBright algorithm.

### Future research

Our search for a candidate biomimetic algorithm was short and successful. Other algorithms may exist and should be explored. Furthermore, the experiment confirms biomimesis of QB only insofar as human processing can perform with the same information. A stronger test for biomimesis is to create situations, where QB is likely to fail, for example, when strong reflections distort the perceived brightness distribution, or when the eye is highly asymmetric. Also, QB ignores any curvature, which produces biases in extreme eyeball positions. If the device is biomimetic, similar biases should be observable.

### Conclusion
